This ZenPack provides support to model OSI Layer 2 (or data link layer) topology. Than that topology information is used to suppress events from devices connection to which was lost because they are connected to broken devices.  Data collection is performed using SNMP.

== Gallery ==
<gallery widths=250px heights=127px>
CDPLLDPDiscover.PNG
ClientMACs.PNG
NeighborSwitches.PNG
</gallery>

== Features ==

The features added by this ZenPack can be summarized as follows. They are each detailed further below.

* Discovery and periodic remodeling of Neighbor Switches using CDP/LLDP.
* Discovery and periodic remodeling of MAC address or forwarding tables.
* Event suppression based on discovered forwarding table information.

=== Discovered Components ===

Assigning the ''zenoss.snmp.CDPLLDPDiscover'' modeler plugin to device(s) will result in SNMP discovery of neighbor switches using a combination of CISCO-CDP-MIB and LLDP-MIB. The discovered neighbor switch information will be shown as ''Neighbor Switches'' in the device's component list.

Assigning the ''zenoss.snmp.ClientMACs'' modeler plugin to device(s) will result in SNMP discovery of the device's forwarding tables using BRIDGE-MIB. This information will be stored on existing ''Network Interfaces'', and won't
result in any new components being created.

=== Monitoring ===

This ZenPack performs no monitoring.

=== Event Suppression ===

This ZenPack supports two types of event suppression.

* Suppression of ping failures when one or more upstream ping failures can be identified as the reason for the failure.
* Suppression of non-ping events on devices with open ping failure events.

We will use the term ''symptomatic event'' to refer to events that are a symptom of a problem, but not the root cause.

==== Ping Event Suppression ====

Suppression of ping events can be enabled on a per-device or device class basis by enabling the zL2SuppressIfPathsDown configuration property. This mode of suppression requires that the ''zenoss.snmp.ClientMACs'' modeler plugin be enabled and successfully functioning on all network devices such as switches and routers that you believe could be a root cause of other monitored devices becoming unreachable.

There are two ways symptomatic ping events can be suppressed. By manually configuring the ultimate gateway(s) of the device(s) using the zL2Gateways property, or by leaving the zL2Gateways property empty and setting the zL2PotentialRootCause property appropriately so that the gateway(s) can be automatically discovered.

[[File:layer2_datacenter_topology_diagram.png|Data Center Topology Diagram]]

The diagram above depicts a common data center network topology where each rack has a redundant pair of access switches sometimes referred to as top-of-rack switches. Each of those top-of-rack switches connect to a redundant pair of end-of-row switches. Each of those end-of-row switches connect to a redundant pair of core switches for the data center. Then perhaps the pair of core switches connect to a pair of gateway routers to connect the data center to the Internet or other data centers over private links. In this kind of topology the layer 3 gateway for hosts is often the core switches.

In this type of topology the gateways for host-1-1-1 can be automatically discovered to be rack-1-1a and rack-1-1b if zL2PotentialRootCause is enabled for the switches, and disabled for the hosts. zL2PotentialRootCause should be enabled for devices that could potentially be a root cause for other devices becoming unreachable, and disabled for devices that cannot be a root cause. This property is important to prevent root caused events from incorrectly being suppressed.

By relying on this automatic discovery of gateways we can only achieve suppression of events from the hosts. We'd get all of the host events suppressed in the case of an entire data center outage, but all of the rack, row, core, and gateway events would remain unsuppressed and it would be left as manual identification of the gateways as the root cause.

To achieve multi-hop suppression the zL2Gateways property must be configured. Despite the name of the property containing "L2", the configured gateways need not be restricted to the layer 2 gateways. In the example topology above, the best value for zL2Gateways would likely be gw-a and gw-b (one per line). It's important to use the Zenoss device id(s) for the gateways, and to enter one per line in zL2Gateways. There's no limit to the number of gateways, but more than two probably isn't realistic.

With zL2Gateways set to gw-a and gw-b in the above topology, a complete failure of the data center would result in all events being suppressed except for two events: a ping failure on each of gw-a and gw-b. This is assuming that zL2SuppressIfDeviceDown is enabled. See ''Non-Ping Event Suppression' below for more information on zL2SuppressIfDeviceDown.

==== Non-Ping Event Suppression ====

Suppression of non-ping events can be enabled on a per-device or device class basis by enabling the zL2SuppressIfDeviceDown configuration property. No other configuration or modeling is necessary. Events will only be suppressed for a device with this property enabled when they have a new, acknowledged, or suppressed critical event in the /Status/Ping event class. This suppression is effective at reducing the potential clutter of symptomatic events when a device is no longer reachable on the network either because it has failed, or because the Zenoss collector is no longer able to reach it.

This suppression can be used together with ping event suppression for the most complete reduction of symptomatic event clutter.

==== Event Suppression Performance ====

All forms of event suppression as described above have a cost in terms of event processing performance. When zL2SuppressIfDeviceDown is enabled, there is a small additional overhead for processing all events. When zL2SuppressIfPathsDown is enabled and first-hop suppression is performed using either automatic gateway discovery or manual gateway configuration, there is another small overhead for processing ping failure events.

In worst case scenario testing the overhead for processing non-ping events with the zL2SuppressIfDeviceDown configuration is approximately TODO percent, and the overhead for processing ping failure events is approximately TODO percent in the case of a first-hop switch failure, and TODO percent in the case of a fourth-hop gateway failure.

All suppression is performed by an event plugin executed within zeneventd processes. Given that zeneventd can be simply scaled by adding more workers/instances, this additional event processing overhead can be offset by running one or more zeneventd instances as event processing throughput needs require.

== Network map ==
[[File:Network map.png|thumb|320px|Network map]]

To inspect interconnections between devices it is possible to use a network map page which is accessed by "Infrastructure" -> "Network Map" tab or "Network Map" menu item of the opened device. It shows connections between devices, interfaces, networks and other entities. In addition, it shows device status (Color of the event with the highest severity for that device).

To display a network map the form on the left side of network map should be filled. "Device ID" field with name or ID of the device starting from which the map will be explored. The depth of the map means how many hops to do when exploring the map. Also, it is possible to select network layers which should be visible on the map. By default all of them will be displayed. To apply changes or create new network map, press "Apply" button at the bottom of the form.

Right mouse click above node opens context menu with options available. 'Pin down' option allows to unpin chosen node.
Detailed information about device is available by clicking on 'Device Info' option from context menu. To draw network map in scope of another node please choose 'Put map root here' option from context menu. It is possible to open device or device component by clicking on corresponding target of network map.

Map could be zoomed using mouse wheel, and panned using mouse. If it disappears from the view, it could be brought back, using "Center" button in the top right corner of the map. There is also zoom level indicator, and color legend.

Please note: for performance reasons map displays only first 1000 nodes found. If this case occurs, filters can help to refine results.

Also note: Sometimes devices will have non-routable MAC interfaces that are not useful. To remove these unwanted or internal MAC addresses from the map, you can add entries to the zLocalMacAddresses property and remodel your network devices.

=== VLANs ===
Network map could have edges that belong not only to Layer2 in general, but to some VLAN. This is links from VLAN-aware switch to its interfaces, and to it's client MAC addresses. This edges could be filtered by their VLAN.

When you select some VLANs on Layers panel, edges that belong to VLANs not included in selection will be removed from map. Layer2 edges without VLAN or edges that have VLAN that is selected will be shown.

== zenmapper daemon ==
To update catalog with connections for network map, zenmapper daemon is used. It runs every 5 minutes by default, but this option could be changed by passing desired number of seconds to the <code>--cycletime</code> argument.

By default zenmapper configured to start 2 workers. This may be changed in config file
by setting "workers" option value. Consider to use more than 2 workers in case you have >1000 devices monitored in Zenoss system. In small or test environment one may disable workers by setting it's value to 0. This affects memory used by zenmapper as well as speed of indexing L2 connections.

zenmapper connects to the ZODB and indexes all the connections provided from providers in ZODB catalog. On 4.2.x RM, running zenmapper on the remote collectors will do nothing because zenmapper runs against the hub. If desired, the additional zenmapper can be disabled by updating <code>/opt/zenoss/etc/daemon.txt</code> on the remote collector.

== Writing your own connection provider ==
Imagine, for example that we want to display on the network map connections of VMware NSX components. They are modeled in NSX ZenPack.

We need to create new class, called for example NSXConnectionsProvider, which inherit from BaseConnectionsProvider, like this:

<syntaxhighlight lang=python>
# our provider will inherit from this:
from ZenPacks.zenoss.Layer2.connections_provider import BaseConnectionsProvider

# and will yield this:
from ZenPacks.zenoss.Layer2.connections_provider import Connection

class NSXConnectionsProvider(BaseConnectionsProvider):
    def get_connections(self):
        # self.context is a entity for which we will provide connections
        for switch in self.context.nsxvirtualSwitchs():
            # so, our device is called NSXManager, and it has switches
            # yield connections to the switches
            yield Connection(self.context, (switch, ), ('layer3', 'nsx'))

            # each switch has interfaces:
            for i in switch.nsxinterfaces():
                # yield connection to the interfaces
                yield Connection(switch, (i, ), ['layer3', 'nsx'])

                # and each interface has many to one connection to edges:
                yield Connection(i, (i.nsxedge(), ), ['layer3', 'nsx'])
</syntaxhighlight>

So, we described how to get connections, now we need to tell zenoss, that this will be connections provider for any NSXManager devices. We do it by registering adapter in our ZenPack's <code>configure.zcml</code>:


<syntaxhighlight lang=xml>
<configure zcml:condition="installed ZenPacks.zenoss.Layer2.connections_provider">
    <!-- Add this adapters only when module connections_provider is possible to import
         (Which means that there is installed recent version of Layer2). -->
    <adapter
        factory=".connections_provider.NSXConnectionsProvider"
        for="ZenPacks.zenoss.NSX.NSXManager.NSXManager"
        provides="ZenPacks.zenoss.Layer2.connections_provider.IConnectionsProvider"
    />
</configure>
</syntaxhighlight>

Another way to include adapters, is to put them in separate file, called for example <code>layer2.zcml</code>:

<syntaxhighlight lang=xml>
<?xml version = "1.0" encoding = "utf-8"?>
<configure
    xmlns="http://namespaces.zope.org/zope"
    xmlns:zcml="http://namespaces.zope.org/zcml"
    >

	<adapter
        factory=".connections_provider.DeviceConnectionsProvider"
        for=".HyperVVSMS.HyperVVSMS"
        provides="ZenPacks.zenoss.Layer2.connections_provider.IConnectionsProvider"
	    />

</configure>
</syntaxhighlight>

and than include that file conditionally:

<syntaxhighlight lang=xml>
    <include file="layer2.zcml"
             xmlns:zcml="http://namespaces.zope.org/zcml"
             zcml:condition="installed ZenPacks.zenoss.Layer2.connections_provider" />
</syntaxhighlight>

To test connections that your provider yields, you could run

zenmapper run -v10 -d <name or id of your modeled device>

And then look it up on the network map.

== Usage ==

This ZenPack has two separate capabilities. The first is to collect clients connected to switch ports so that event suppression can be done when the switch fails, and the second is to discover neighbor relationships between network devices using the CDP (Cisco Discovery Protocol) and LLDP (Link Layer Discover Protocol).

=== Collecting Switch Port Clients ===

To enable discovery of clients connected to switch ports you must enable the <code>zenoss.snmp.ClientMACs</code> modeler plugin for the switch devices. There is no
need to enable this plugin for hosts, servers, or other endpoint devices. It is recommended to only assign the modeler plugin to access switch to which monitored servers are connected.

The discovery is done using BRIDGE-MIB forwarding tables, so it's a prerequisite that the switch supports BRIDGE-MIB.

=== Collecting Network Device Neighbors ===

To collect neighbor information from network devices that support CDP or LLDP, you must enable the <code>zenoss.snmp.CDPLLDPDiscover</code> modeler plugin for the devices.



This ZenPack has the following requirements.

;[[ZenPack:PythonCollector|PythonCollector ZenPack]]
: This ZenPack depends on [[ZenPack:PythonCollector|PythonCollector]] being installed, and having the associated <code>zenpython</code> collector process running.

== Service Impact ==

When combined with the Zenoss Service Dynamics product, this ZenPack adds
built-in service impact capability based on Layer 2 data. The following service
impact relationships are automatically added. These will be included in any
services that contain one or more of the explicitly mentioned entities.

;Service Impact Relationships
* Device impacted by upstream switch device.


== Troubleshooting ==

=== Europa ===
If you are re-installing or updating this ZenPack on Europa, you should first check in control center that <code>zenmapper</code> daemon is stopped, and if not - stop it. It should be stopped automatically, but while this issue is not fixed, you should do that by hand.

=== Open vSwitch zenpack ===
Open vSwitch zenpack version prior to 1.1.1 should be updated or removed before Layer2 zenpack
installation.

=== Empty map/links for device ===
In case index for certain device is broken, one may force zenmapper to reindex this specific device. Daemon should be runed with <code>--force</code> option.

=== Layer2 forwarding table ===
Let's discuss Layer2 connections in particular.

The essential mechanism that distinguishes network switches from network
hubs is the MAC forwarding table. Instead of broadcasting incoming link
layer frames to all it's interfaces, as hubs do, switches look into the
forwarding table to find out which particular interface is connected to the
destination device. The switch learns which devices are connected to which
interface by looking at the source MAC address of incoming frames. Those
MAC addresses are called "client MAC addresses".

For zenoss to discover Layer 2 connection between some devices, MAC address of
some interface of one device should be equal to client MAC address of some
interface of other device. You could check if client MAC addresses for
interface are modeled by looking at it's "Clients MAC addresses" display.
It there are none, check that <code>zenoss.snmp.ClientMACs</code> modeler
plugin is bound to device, and remodel device.

It is also possible that there are no MAC address required to discover
connection in forwarding table. To check that, you could run debug utility
[https://github.com/zenoss/ZenPacks.zenoss.Layer2/blob/develop/bridge_snmp.py bridge_snmp.py]:

<syntaxhighlight lang=bash>
    python bridge_snmp.py clientmacs -c <community_string> <host>
</syntaxhighlight>

and see if your client mac address is visible at switch at all.

Records in forwarding table are aged pretty fast, by default in 5 minutes. So,
when there were no network activity on connection for more than 5 minutes,
entry will be removed from switch forwarding table. You could check
<code>dot1dTpAgingTime</code> object to know exact timeout period in seconds:

<syntaxhighlight lang=bash>
$ snmpget -v2c -c <community_string> <host> 1.3.6.1.2.1.17.4.2.0
SNMPv2-SMI::mib-2.17.4.2.0 = INTEGER: 300
</syntaxhighlight>


=== Impact ===
This ZenPack also adds impact relation for layer2 connections. Switches impact
devices connected to them. But this will work only when such connection is
present on network map (see two previous sections for guide on troubleshooting
that).

If there is connection on network map, but still, no impact relation, than,
probably impact relations were not rebuilt. You could do that by indexing
device, for example by changing some field on overview and saving it. Or
modeling device again.


=== Limitations ===
There are no client MACs data on interfaces modeled for the first time.
This happens because <code>zenoss.snmp.ClientMACs</code> plugin runs before
interfaces are modeled by another network modeler plugin (for example
<code>cisco.snmp.Interfaces</code> or <code>zenoss.snmp.InterfaceMap</code>),
so there is no entities to save this attribute on. Currently it is not possible
to define order of modeler execution, so this remains a limitation.

Possible workaround is to wait for next model cycle or just model the device again manually.

=== More information ===
If you cannot find the answer in the documentation, then Resource Manager (Service Dynamics)
users should contact [https://support.zenoss.com Zenoss Customer Support].
Core users can use the #zenoss IRC channel or the community.zenoss.org forums.


== Installed Items ==

Installing this ZenPack will add the following items to your Zenoss system.

'''Modeler Plugins'''
* zenoss.snmp.CDPLLDPDiscover
* zenoss.snmp.ClientMACs

'''zProperties'''
* zL2Gateways (default: [])
* zL2PotentialRootCause (default: True)
* zL2SuppressIfDeviceDown (default: False)
* zL2SuppressIfPathsDown (default: False)
* zLocalMacAddresses (default: ["00:00:00:00:00:00"])
* zZenossGateway (deprecated by zL2Gateways)

'''Daemons'''
* zenmapper

== Changes ==

;1.3.0
* Support for multiple gateways per device or device class. (ZEN-24767)
* Added zL2Gateways property. (ZEN-24767)
* Deprecated zZenossGateway property. (ZEN-24767)
* Added zL2PotentialRootCause to allow automatic gateways discovery.
* Added zL2SuppressIfPathsDown to toggle ping event suppression.
* Added zL2SuppressIfDeviceDown to toggle non-ping event suppression.
* Added rootCauses event field for suppressed events.
* Improved event suppression performance and reliability.
* Added zLocalMacAddresses to remove unwanted interfaces in maps. (ZEN-23182)
* Added client discovery support using Q-BRIDGE MIB. (ZEN-25336)

;1.2.2
* Fix potential 2 minute modeling delay in Zenoss 4.
* Fix "Connection refused" when Redis not available.

;1.2.1
* Added "workers" option to zenmapper daemon.
* Refactored connection catalog to use Redis as a storage. This prevent from cases
where ZoDB grows over time (ZEN-22834).
* Layer2 index now don't touch/modify ZoDB storage in any of cases.
* Devices added to index in time they changed. Zenmapper daemon adds to index only
differences, e.g. indexing is incremental now.
* In time when zenpack installed/upgraded zenmapper daemon will create initial index.
This occurs only on first run. And it may take several minutes depending on
number of devices.

;1.1.1
* Fix page help code in Layer2 ZP conflict with other ZenPacks (ZEN-21264)

;1.1.0
* When filtering by VLAN show also layer2 links that are VLAN-unaware (ZEN-20946)
* Add checkbox that allows to show full map
* Fix Cisco community string indexing in ClientMACs modeler plugin.
* Fix issue getting client MAC address from labeled VLAN interfaces. (ZEN-19874)
* Fix Network Map - Missing link from Cisco device to subnet on depth 2,3,4 (ZEN-18603)
* Make Impact use new connections catalog instead of macs catalog (ZEN-18636)
* Fix Broken link for Subnet node in Network map (ZEN-20749)

;1.0.3
* Remove macs_catalog when removing the ZenPack. (ZEN-17967)
* Replace Layer2Info template with ClientMACs modeler plugin.

;1.0.2
* Fix modeling of CDP neighbor switches with IPv6 addresses. (ZEN-17248)
* Avoid community@VLAN context querying for non-Cisco switches. (ZEN-17258)
* Change default cycletime for Layer2Info from 30 minutes to 12 hours. (ZEN-17031)

;1.0.1
* Fix device overview links error. (ZEN-14063)
* Remove add/remove from catalog logging. (ZEN-15465)
* Fix usage of incorrect community VLAN suffixes on BRIDGE-MIB queries. (ZEN-16951)
* Fix looping of impact relationships between switches. (ZEN-17020)
* Fix incorrect modeling of neighbor switches and improve modeling time. (ZEN-17023)
* Stop binding Layer2Info template to /Network by default. (ZEN-17035)

;1.0.0
* Initial release
